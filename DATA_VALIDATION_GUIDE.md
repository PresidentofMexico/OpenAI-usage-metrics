# Weekly and Monthly ChatGPT Data Validation System

## Overview

This system provides comprehensive tools for managing, validating, and analyzing weekly and monthly ChatGPT usage data exports from OpenAI Enterprise.

## Components

### 1. ChatGPT Data Validator (`chatgpt_data_validator.py`)

Validates that weekly data sums correctly to monthly totals across all message categories.

**Key Features:**
- ‚úÖ Validates all message categories (GPT, Tool, Project, General messages)
- ‚úÖ Generates detailed validation reports in text and JSON formats
- ‚úÖ Identifies discrepancies between weekly and monthly data
- ‚úÖ Validates that category breakdowns sum to total messages
- ‚úÖ Configurable tolerance percentage (default: 1%)

**Usage:**

```python
from chatgpt_data_validator import ChatGPTDataValidator

# Initialize validator
validator = ChatGPTDataValidator(tolerance_percentage=1.0)

# Validate weekly files against monthly file
weekly_files = ['week1.csv', 'week2.csv', 'week3.csv', 'week4.csv']
monthly_file = 'monthly_report.csv'

results = validator.validate_weekly_to_monthly(weekly_files, monthly_file)

# Generate report
print(validator.generate_report(results, output_format='text'))

# Save reports
validator.save_report(results, 'validation_report.txt', output_format='text')
validator.save_report(results, 'validation_report.json', output_format='json')
```

**Command Line Usage:**

```bash
# Run with sample data
python chatgpt_data_validator.py

# The script validates sample_weekly_data.csv against sample_monthly_data.csv
# and generates validation_report.txt and validation_report.json
```

### 2. Data Handlers (`data_handlers.py`)

Provides separate, specialized handlers for analyzing weekly and monthly data.

#### WeeklyDataHandler

Analyzes weekly patterns, trends, and peak usage weeks.

**Features:**
- Weekly trend analysis
- Peak week identification
- User engagement patterns
- Week-over-week comparisons

**Usage:**

```python
from data_handlers import WeeklyDataHandler

# Load weekly data
handler = WeeklyDataHandler()
handler.load_data(['week1.csv', 'week2.csv', 'week3.csv'])

# Analyze trends
trends = handler.analyze_trends()
print(f"Average weekly messages: {trends['summary']['avg_weekly_messages']}")

# Identify peak weeks
peak_weeks = handler.identify_peak_weeks(top_n=5)
for week in peak_weeks:
    print(f"{week['week_start']}: {week['total_messages']} messages")

# Analyze user engagement
engagement = handler.analyze_user_engagement()
print(f"Average engagement rate: {engagement['avg_engagement_rate']}%")
```

#### MonthlyDataHandler

Provides monthly projections, quarterly summaries, and seasonality analysis.

**Features:**
- Annual usage projections
- Quarterly trend analysis
- Seasonality detection
- Growth rate calculations

**Usage:**

```python
from data_handlers import MonthlyDataHandler

# Load monthly data
handler = MonthlyDataHandler()
handler.load_data(['jan.csv', 'feb.csv', 'mar.csv'])

# Project annual usage
projection = handler.project_annual_usage()
print(f"Simple annual projection: {projection['projections']['simple_annual']}")
print(f"Growth-adjusted projection: {projection['projections']['growth_adjusted_annual']}")

# Analyze quarterly trends
quarterly = handler.analyze_quarterly_trends()
print(f"Total quarters analyzed: {quarterly['total_quarters']}")

# Analyze seasonality
seasonality = handler.analyze_seasonality()
print(f"Peak month: {seasonality['peak_month']['name']}")
print(f"Low month: {seasonality['low_month']['name']}")
```

#### DataReconciliationTool

Ensures consistency between weekly and monthly data.

**Features:**
- Automatic reconciliation of weekly/monthly data
- Category-level validation
- Configurable tolerance levels
- Detailed reconciliation reports

**Usage:**

```python
from data_handlers import WeeklyDataHandler, MonthlyDataHandler, DataReconciliationTool

# Load both datasets
weekly = WeeklyDataHandler()
weekly.load_data(['week1.csv', 'week2.csv', 'week3.csv', 'week4.csv'])

monthly = MonthlyDataHandler()
monthly.load_data(['monthly_report.csv'])

# Reconcile data
reconciler = DataReconciliationTool()
results = reconciler.reconcile(weekly, monthly, tolerance_pct=1.0)

# Generate report
print(reconciler.generate_reconciliation_report(results))
```

## Sample Data Files

### `sample_monthly_data.csv`

Contains example monthly usage data for 5 users across May 2025:
- 750 total messages
- 58 GPT messages
- 370 tool messages
- 108 project messages

### `sample_weekly_data.csv`

Contains corresponding weekly data broken into 4 weeks that sum to the monthly totals:
- Week 1 (May 1-7): 255 messages
- Week 2 (May 8-14): 248 messages
- Week 3 (May 15-21): 128 messages
- Week 4 (May 22-28): 119 messages

**Validation Result:** ‚úÖ All weekly data sums correctly to monthly totals

## Expected CSV Format

Both weekly and monthly files use the same OpenAI export format:

```csv
cadence,period_start,period_end,email,name,department,is_active,
messages,gpt_messages,tool_messages,project_messages,...
```

**Key Columns:**
- `cadence`: "Weekly" or "Monthly"
- `period_start`: Start date of the period (YYYY-MM-DD)
- `period_end`: End date of the period (YYYY-MM-DD)
- `email`: User email address
- `name`: User display name
- `department`: Department (JSON array format)
- `is_active`: 1 if user was active, 0 if not
- `messages`: Total message count
- `gpt_messages`: Custom GPT message count
- `tool_messages`: Tool usage count (Code Interpreter, etc.)
- `project_messages`: Project-based message count

## Testing

### Run All Tests

```bash
python tests/test_data_validation.py
```

### Test Coverage

The test suite validates:
1. ‚úÖ Validator initialization
2. ‚úÖ Sample data loading
3. ‚úÖ Weekly to monthly validation
4. ‚úÖ Category breakdown validation
5. ‚úÖ Report generation (text and JSON)
6. ‚úÖ WeeklyDataHandler functionality
7. ‚úÖ MonthlyDataHandler functionality
8. ‚úÖ DataReconciliationTool functionality
9. ‚úÖ Discrepancy detection

**Expected Output:**
```
üìà Total: 9/9 tests passed
üéâ All tests passed successfully!
```

## Validation Reports

### Text Report Format

```
================================================================================
ChatGPT Data Validation Report
================================================================================
Generated: 2025-10-30T17:10:36
Monthly File: sample_monthly_data.csv
Period: 2025-05-01 to 2025-05-31
Weekly Files: 1
Tolerance: 1.0%

Overall Status: ‚úÖ PASS

Category Validation Results:
--------------------------------------------------------------------------------

‚úÖ MESSAGES
  Status: PASS
  Total Users: 5
  Matching Users: 5
  Discrepancies: 0
  Weekly Sum: 750
  Monthly Sum: 750
  Overall Difference: +0
```

### JSON Report Format

```json
{
  "timestamp": "2025-10-30T17:10:36.377281",
  "monthly_file": "sample_monthly_data.csv",
  "period_start": "2025-05-01",
  "period_end": "2025-05-31",
  "overall_status": "PASS",
  "categories": {
    "messages": {
      "status": "PASS",
      "total_users": 5,
      "matching_users": 5,
      "discrepancy_users": 0,
      "weekly_sum": 750,
      "monthly_sum": 750,
      "overall_difference": 0
    }
  }
}
```

## Integration with Existing Dashboard

These tools can be integrated with the main Streamlit dashboard:

1. **Upload Validation Tab**: Add a new tab for uploading and validating weekly/monthly data
2. **Automated Checks**: Run validation automatically when processing OpenAI CSV files
3. **Trend Analysis**: Display weekly trends and monthly projections in the analytics tabs
4. **Data Quality Alerts**: Show warnings when discrepancies are detected

### Example Integration

```python
import streamlit as st
from chatgpt_data_validator import ChatGPTDataValidator

# In your Streamlit app
st.header("Data Validation")

if st.button("Validate Weekly Data"):
    validator = ChatGPTDataValidator()
    results = validator.validate_weekly_to_monthly(
        weekly_files=get_weekly_files(),
        monthly_file=get_monthly_file()
    )
    
    if results['overall_status'] == 'PASS':
        st.success("‚úÖ All data validated successfully!")
    else:
        st.error("‚ùå Validation failed - see details below")
    
    st.json(results)
```

## Best Practices

### Data Validation
- ‚úÖ Run validation checks before importing data into the dashboard
- ‚úÖ Use 1% tolerance for production, stricter for testing
- ‚úÖ Save validation reports for audit trails
- ‚úÖ Review discrepancies before accepting data

### Weekly Analysis
- ‚úÖ Analyze weekly trends to identify usage patterns
- ‚úÖ Track week-over-week growth for early insights
- ‚úÖ Identify peak weeks for capacity planning
- ‚úÖ Monitor user engagement rates weekly

### Monthly Analysis
- ‚úÖ Use monthly data for strategic planning
- ‚úÖ Generate quarterly reports for executive reviews
- ‚úÖ Project annual usage for budget planning
- ‚úÖ Track seasonality for resource allocation

### Reconciliation
- ‚úÖ Reconcile weekly and monthly data monthly
- ‚úÖ Investigate any discrepancies immediately
- ‚úÖ Document validation results
- ‚úÖ Keep reconciliation reports for compliance

## Troubleshooting

### Common Issues

**Issue: "No valid data found after processing"**
- Ensure CSV files have the correct format
- Check that `is_active` column exists
- Verify date columns are in YYYY-MM-DD format

**Issue: "Discrepancies detected"**
- Review the specific users with mismatches
- Check for duplicate entries in weekly data
- Verify that weekly periods don't overlap
- Ensure all weeks in the month are included

**Issue: "File not found"**
- Verify file paths are correct
- Use absolute paths or ensure working directory is correct
- Check file permissions

### Debug Mode

Enable verbose output for troubleshooting:

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Your validation code here
```

## Performance

### Benchmark Results

- Small datasets (5-10 users, 4 weeks): < 1 second
- Medium datasets (50-100 users, 4 weeks): 1-3 seconds
- Large datasets (500+ users, 4 weeks): 5-10 seconds

### Optimization Tips

- Load only necessary columns for analysis
- Filter inactive users early in the pipeline
- Use batch processing for multiple months
- Cache results when processing repeated files

## Future Enhancements

Planned features for future releases:

- [ ] Support for Excel files (.xlsx)
- [ ] Automated email alerts for discrepancies
- [ ] Machine learning-based anomaly detection
- [ ] Interactive visualization of trends
- [ ] API endpoints for programmatic access
- [ ] Support for other AI platforms (Claude, Gemini)
- [ ] Historical trend comparison
- [ ] Predictive usage forecasting

## Support

For issues or questions:
1. Check this documentation
2. Review sample data files
3. Run test suite to verify installation
4. Check GitHub issues for similar problems
5. Create a new issue with error details

## License

This validation system is part of the OpenAI Usage Metrics Dashboard project.
